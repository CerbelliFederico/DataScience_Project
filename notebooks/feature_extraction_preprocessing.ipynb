{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Runned using python 3.12.1\n",
    "\n",
    "## Importing Libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "#%pip install librosa\n",
    "import librosa\n",
    "\n",
    "# Load data, we use the ID column as the index, so we do not need to drop it\n",
    "train_data = pd.read_csv('DSL_Winter_Project_2025/development.csv', index_col=0)         # no need to skip rows\n",
    "evaluation_data = pd.read_csv('DSL_Winter_Project_2025/evaluation.csv', index_col=0)\n",
    "\n",
    "# Concatenate 'DSL_Winter_Project_2025/' with the existing paths in the df 'path' column\n",
    "train_data['path'] = 'DSL_Winter_Project_2025/' + train_data['path']\n",
    "evaluation_data['path'] = 'DSL_Winter_Project_2025/' + evaluation_data['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing:\n",
    "# 1. convert the 'tempo' column from string to float\n",
    "# 2. encode categrical columns\n",
    "# 3. extract features from the audio files\n",
    "# 4. drop columns: path, sampling_rate (constant), num_characters (higly correlated with num_words)\n",
    "\n",
    "# 1.\n",
    "train_preprocessing = train_data.drop(columns=['age'])      # drop the 'age' column since it is not present in the evaluation data and we could not combine the two datasets\n",
    "combined_data = pd.concat([train_preprocessing, evaluation_data], axis=0).reset_index(drop=True)            # https://pandas.pydata.org/docs/reference/api/pandas.concat.html\n",
    "combined_data['tempo'] = ((combined_data['tempo'].str.replace('[', '')).str.replace(']', '')).astype(float) # https://pandas.pydata.org/docs/reference/api/pandas.Series.str.replace.html\n",
    "\n",
    "# 2.\n",
    "# Since we noted many ethnicities have few samples, we can group them into a single category 'Other' before applying one-hot encoding. This way we can reduce the dimensionality.\n",
    "ethnicity_counts = train_data['ethnicity'].value_counts()\n",
    "frequent_ethnicities = ethnicity_counts[ethnicity_counts > 100].index\n",
    "combined_data['ethnicity'] = combined_data['ethnicity'].map(lambda x: x if x in frequent_ethnicities else 'Other')\n",
    "\n",
    "# Correct the typo in the 'gender' column\n",
    "combined_data.loc[combined_data['gender'] == 'famale', 'gender'] = 'female'\n",
    "\n",
    "columns_to_encode = ['gender', 'ethnicity']\n",
    "combined_data = pd.get_dummies(combined_data, columns=columns_to_encode)    # one-hot encoding: # https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.\n",
    "\n",
    "## Define a frequency filter\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "sr = 22050\n",
    "\n",
    "# Define the cutoff frequency for the low pass filter,\n",
    "# we cut at 4500 Hz to avoid losing too much information\n",
    "# NOTE: The cutoff frequency can be defined in Hz, but we need to specify the sr as an argument in the butter function\n",
    "cutoff_frequency = 4500         # gain = -3dB \n",
    "\n",
    "# Define the low pass filter (precompute the coefficients)\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.butter.html#butter\n",
    "b, a = butter(Wn=cutoff_frequency, fs=sr, N=5) # N = order\n",
    "\n",
    "# Apply the low pass filter to the audio signal\n",
    "def lowpass_filter(data, b=b, a=a):\n",
    "    y = lfilter(b, a, data)\n",
    "    return y # filtered signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature extraction\n",
    "def extract_audio_features(file_path, sr=sr):\n",
    "    try:\n",
    "        # Load audio file\n",
    "        # https://librosa.org/doc/latest/core.html#audio-loading: load audio file as floating point time series\n",
    "        y, sr= librosa.load(file_path, sr=sr)    # sr = sampling rate\n",
    "\n",
    "        # apply the low pass filter\n",
    "        y = lowpass_filter(y)\n",
    "\n",
    "        # Check if the audio is empty\n",
    "        if np.all(y == 0):\n",
    "            print(f\"Audio vuoto: {file_path}\")\n",
    "            return None\n",
    "\n",
    "        # Set the window size to 0.74 seconds, except when the time series do not have enough samples\n",
    "        n_fft = min(16384, len(y)) # the choice of a power of 2 is due to the FFT algorithm (improve effciency)\n",
    "        if n_fft < 16384:\n",
    "            print(f\"Audio equal to {n_fft} samples < 16384: {file_path}\")\n",
    "        \n",
    "        # Mel Frequency Cepstral Coefficients (MFCCs) \n",
    "        # https://librosa.org/doc/latest/generated/librosa.feature.mfcc.html#librosa-feature-mfcc\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, n_fft=n_fft)                               # returns an array of size (n_mfcc, t) where t is the number of frames (each window computation is a frame)\n",
    "        mfccs_mean = np.mean(mfccs, axis=1)                                                            # compute the mean of each MFCC coefficient over all frames\n",
    "\n",
    "        # Chromagram\n",
    "        # https://librosa.org/doc/latest/generated/librosa.feature.chroma_stft.html#librosa-feature-chroma-stft\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_fft=n_fft, hop_length = int(n_fft/2))\n",
    "        chroma_mean = np.mean(chroma, axis=1)\n",
    "\n",
    "\n",
    "        # Dizionario con le caratteristiche estratte\n",
    "        features = {\n",
    "            'mfcc_1': mfccs_mean[0],\n",
    "            'mfcc_2': mfccs_mean[1],\n",
    "            'mfcc_3': mfccs_mean[2],\n",
    "            'mfcc_4': mfccs_mean[3],\n",
    "            'mfcc_5': mfccs_mean[4],\n",
    "            'mfcc_6': mfccs_mean[5],\n",
    "            'mfcc_7': mfccs_mean[6],\n",
    "            'mfcc_8': mfccs_mean[7],\n",
    "            'mfcc_9': mfccs_mean[8],\n",
    "            'mfcc_10': mfccs_mean[9],\n",
    "            'mfcc_11': mfccs_mean[10],\n",
    "            'mfcc_12': mfccs_mean[11],\n",
    "            'mfcc_13': mfccs_mean[12],\n",
    "            'chroma_1': chroma_mean[0],\n",
    "            'chroma_2': chroma_mean[1],\n",
    "            'chroma_3': chroma_mean[2],\n",
    "            'chroma_4': chroma_mean[3],\n",
    "            'chroma_5': chroma_mean[4],\n",
    "            'chroma_6': chroma_mean[5],\n",
    "            'chroma_7': chroma_mean[6],\n",
    "            'chroma_8': chroma_mean[7],\n",
    "            'chroma_9': chroma_mean[8],\n",
    "            'chroma_10': chroma_mean[9],\n",
    "            'chroma_11': chroma_mean[10],\n",
    "            'chroma_12': chroma_mean[11],\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    return features\n",
    "\n",
    "# List to store the extracted audio features\n",
    "audio_features = []\n",
    "\n",
    "for file_path in combined_data['path']:\n",
    "    features = extract_audio_features(file_path)\n",
    "    audio_features.append(features)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_audio_features = pd.DataFrame(audio_features)\n",
    "\n",
    "# Concatenate the extracted audio features to the DataFrame\n",
    "combined_data = pd.concat([combined_data.reset_index(drop=True), df_audio_features.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.\n",
    "columns_to_drop = ['path', 'sampling_rate', 'num_characters', 'min_pitch', 'max_pitch']\n",
    "combined_data = combined_data.drop(columns=columns_to_drop)\n",
    "\n",
    "# Split the data back into train and evaluation\n",
    "train_data = pd.concat([combined_data.iloc[:len(train_data)], train_data['age']], axis=1)\n",
    "evaluation_data = combined_data.iloc[len(train_data):].reset_index(drop=True)\n",
    "\n",
    "## Save the dataframe after feature extraction\n",
    "train_data.to_csv('train_data_features_extracted_filtered.csv')\n",
    "evaluation_data.to_csv('evaluation_data_features_extracted_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first rows of the train_data DataFrame\n",
    "pd.set_option('display.max_columns', None)\n",
    "display(train_data.head())\n",
    "\n",
    "# Display the first rows of the evaluation_data DataFrame\n",
    "display(evaluation_data.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
